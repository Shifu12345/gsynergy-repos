{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42285091-2f95-4c4a-a131-b5c9a8f1d9f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create database processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b0727f8-4b4b-42a7-a320-42bbfae1441e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "New Year's Day\t1\t1\n",
    "Valentine's Day\t2\t14\n",
    "St. Patrick's Day\t3\t17\n",
    "Independence Day\t7\t4\n",
    "Halloween\t10\t31\n",
    "Veterans Day\t11\t11\n",
    "Christmas\t12\t25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32b5c515-a7da-471b-8837-256db3ce185b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE IF EXISTS processed_data.dim_date;\n",
    "\n",
    "CREATE TABLE processed_data.dim_date AS \n",
    "SELECT * FROM raw_data.dim_date;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56943470-064f-4e79-bc27-6f460f2fea14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "303578c2-99f7-4a84-8dbc-042cdb8b1373",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#fixed holidays\n",
    "fixed_holidays = {\n",
    "    \"New Year's Day\": (1, 1),\n",
    "    \"Valentine's Day\": (2, 14),\n",
    "    \"Independence Day\": (7, 4),\n",
    "    \"Halloween\": (10, 31),\n",
    "    \"Christmas\": (12, 25),\n",
    "    \"St. Patrick's Day\":(3,17),\n",
    "    \"Veterans Day\":(11,11)\n",
    "}\n",
    "\n",
    "fixed_holiday_list = list(fixed_holidays.keys())\n",
    "\n",
    "df_dim_hldy=spark.sql(\"select * from raw_data.dim_hldy\")\n",
    "\n",
    "#sarrogate key\n",
    "df_dim_hldy = df_dim_hldy.withColumn(\"hldy_id\", monotonically_increasing_id() + 1)\n",
    "\n",
    "df_dim_hldy = df_dim_hldy.withColumn(\n",
    "    \"is_fixed\",\n",
    "    when(col(\"hldy_label\").isin(fixed_holiday_list), lit(\"Yes\")).otherwise(lit(\"No\"))\n",
    ")\n",
    "\n",
    "df_dim_hldy = df_dim_hldy.withColumn(\n",
    "    \"hldy_mnth\",\n",
    "    when(col(\"hldy_label\") == \"New Year's Day\", lit(1))\n",
    "    .when(col(\"hldy_label\") == \"Valentine's Day\", lit(2))\n",
    "    .when(col(\"hldy_label\") == \"Independence Day\", lit(7))\n",
    "    .when(col(\"hldy_label\") == \"Halloween\", lit(10))\n",
    "    .when(col(\"hldy_label\") == \"Christmas\", lit(12))\n",
    "    .when(col(\"hldy_label\") == \"St. Patrick's Day\", lit(3))\n",
    "    .when(col(\"hldy_label\") == \"Veterans Day\", lit(11))\n",
    "    .otherwise(lit(0))\n",
    ")\n",
    "\n",
    "df_dim_hldy = df_dim_hldy.withColumn(\n",
    "    \"hldy_day\",\n",
    "    when(col(\"hldy_label\") == \"New Year's Day\", lit(1))\n",
    "    .when(col(\"hldy_label\") == \"Valentine's Day\", lit(14))\n",
    "    .when(col(\"hldy_label\") == \"Independence Day\", lit(4))\n",
    "    .when(col(\"hldy_label\") == \"Halloween\", lit(31))\n",
    "    .when(col(\"hldy_label\") == \"Christmas\", lit(25))\n",
    "    .when(col(\"hldy_label\") == \"St. Patrick's Day\", lit(17))\n",
    "    .when(col(\"hldy_label\") == \"Veterans Day\", lit(11))\n",
    "    .otherwise(lit(0))\n",
    ")\n",
    "\n",
    "df_dim_hldy = df_dim_hldy.select(\"hldy_id\", \"hldy_label\", \"is_fixed\", \"hldy_mnth\", \"hldy_day\")\n",
    "\n",
    "\n",
    "df_dim_hldy.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5d40d54-2817-4b75-ae76-c854d3668757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "CREATE TABLE IF NOT EXISTS processed_data.dim_hldy (\n",
    "    hldy_id LONG,\n",
    "    hldy_label STRING ,\n",
    "    is_fixed STRING,\n",
    "    hldy_mnth INT,\n",
    "    hldy_day INT\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c435e0b-d1c7-4bcb-832b-2652cd6a8d3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#create dim table\n",
    "df_dim_hldy.write.mode(\"overwrite\").saveAsTable(\"processed_data.dim_hldy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d6bd0ff-5efb-4fb4-a23b-bc729a7ee07b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from processed_data.dim_hldy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fca5bb3e-a0fd-4141-a184-f367a1a7b98a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "#adding dates from 2016 to 2024\n",
    "date_range = pd.date_range(start=\"2016-01-01\", end=\"2024-12-31\")\n",
    "df_dates = spark.createDataFrame(pd.DataFrame({\"date\": date_range}))\n",
    "\n",
    "#Extract necessary date----month----year-----day----dayofweek\n",
    "df_dates = df_dates.withColumn(\"year\", expr(\"year(date)\")) \\\n",
    "                   .withColumn(\"month\", expr(\"month(date)\")) \\\n",
    "                   .withColumn(\"day\", expr(\"day(date)\")) \\\n",
    "                   .withColumn(\"dow\", expr(\"dayofweek(date)\")) \n",
    "\n",
    "#fixed date holidays\n",
    "fixed_holidays = {\n",
    "    \"New_Years_Day\": (1, 1),\n",
    "    \"Independence_Day\": (7, 4),\n",
    "    \"Halloween\": (10, 31),\n",
    "    \"Christmas\": (12, 25),\n",
    "    \"Valentines_Day\": (2, 14),\n",
    "    \"Veterans_Day\": date(11, 11)\n",
    "}\n",
    "\n",
    "#floating holidays-------------logic req\n",
    "def get_floating_holidays(year):\n",
    "    \n",
    "    def nth_weekday(year, month, weekday, nth):\n",
    "        first_day = date(year, month, 1)\n",
    "        first_occurrence = first_day + timedelta(days=(weekday - first_day.weekday() + 7) % 7)\n",
    "        return first_occurrence + timedelta(weeks=(nth - 1))\n",
    "\n",
    "    holidays = {\n",
    "        \"Presidents_Day\": nth_weekday(year, 2, 0, 3),  #3rd Monday of Feb\n",
    "        \"Easter\": date(year, 3, 31),  #Easter calculation needed no fixed date\n",
    "        \"Mothers_Day\": nth_weekday(year, 5, 6, 2),  #2nd Sunday of May\n",
    "        \"Memorial_Day\": nth_weekday(year, 5, 0, -1),  #Last Monday of May\n",
    "        \"Fathers_Day\": nth_weekday(year, 6, 6, 3),  #3rd Sunday of June\n",
    "        \"Labor_Day\": nth_weekday(year, 9, 0, 1),  #      1st Monday of Sep\n",
    "        \"Columbus_Day\": nth_weekday(year, 10, 0, 2),  #       2nd Monday of Oct\n",
    "        \"Election_Day\": nth_weekday(year, 11, 1, 1),  #   1st Tuesday of Nov\n",
    "        \"Thanksgiving\": nth_weekday(year, 11, 3, 4),  #          4th Thursday of Nov\n",
    "        \"Martin_Luther_King_Day\": nth_weekday(year, 1, 0, 3),  # 3rd Monday of Jan\n",
    "    }\n",
    "    \n",
    "    return holidays\n",
    "\n",
    "#-----------holiday mappings df creation-----------------\n",
    "holiday_records = []\n",
    "for year in range(2016, 2025):\n",
    "    holidays = get_floating_holidays(year)\n",
    "    \n",
    "    for hldy, (month, day) in fixed_holidays.items():\n",
    "        holiday_records.append((hldy, f\"{year}-{month:02d}-{day:02d}\"))\n",
    "    \n",
    "    for hldy, hldy_date in holidays.items():\n",
    "        holiday_records.append((hldy, hldy_date.strftime(\"%Y-%m-%d\")))\n",
    "\n",
    "df_hldy = spark.createDataFrame(holiday_records, [\"hldy_label\", \"date\"])\n",
    "\n",
    "#  -----Join with dim_date columns----\n",
    "df_dim_time_hldy = df_dates.join(df_hldy, \"date\", \"left\") \\\n",
    "    .withColumn(\"hldy_id\", expr(\"monotonically_increasing_id()\")) \\\n",
    "    .withColumn(\"is_fixed\", when(col(\"hldy_label\").isin(list(fixed_holidays.keys())), lit(\"Yes\")).otherwise(lit(\"No\")))\n",
    "\n",
    "df_dim_time_hldy.show()\n",
    "df_dim_time_hldy.write.mode(\"overwrite\").saveAsTable(\"processed_data.dim_time_hldy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68f2a3c1-d3fb-4d61-9fa5-55e7c1d0431a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from processed_data.dim_time_hldy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8823f289-d87a-4960-aaae-242964b77784",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from hive_metastore.raw_data.dim_date limit 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abc169ce-ed89-4b97-92ad-4a1cbaa3aeed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table processed_data.dim_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4809d926-9af7-4d51-8c35-2eacf4d2d6f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#new date dimension table from 2016 to 2024----------------\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, year, month, dayofmonth, dayofweek, weekofyear, quarter, expr, lit\n",
    "from pyspark.sql.types import IntegerType\n",
    "import datetime\n",
    "\n",
    "# Generate Date Range from 2016-01-01 to 2024-12-31\n",
    "start_date = datetime.date(2016, 1, 1)\n",
    "end_date = datetime.date(2024, 12, 31)\n",
    "date_list = [(start_date + datetime.timedelta(days=x)) for x in range((end_date - start_date).days + 1)]\n",
    "df = spark.createDataFrame([(d,) for d in date_list], [\"date\"])\n",
    "\n",
    "# Generate Required Columns\n",
    "df = df.withColumn(\"fscldt_id\", date_format(\"date\", \"yyyyMMdd\").cast(IntegerType())) \\\n",
    "       .withColumn(\"fscldt_label\", date_format(\"date\", \"MMM d, yyyy\")) \\\n",
    "       .withColumn(\"fsclwk_id\", expr(\"year(date) * 100 + weekofyear(date)\")) \\\n",
    "       .withColumn(\"fsclwk_label\", expr(\"concat('WK ', lpad(weekofyear(date), 2, '0'), ', ', year(date))\")) \\\n",
    "       .withColumn(\"fsclmth_id\", expr(\"year(date) * 100 + month(date)\")) \\\n",
    "       .withColumn(\"fsclmth_label\", date_format(\"date\", \"MMM, yyyy\")) \\\n",
    "       .withColumn(\"fsclqrtr_id\", expr(\"year(date) * 10 + quarter(date)\")) \\\n",
    "       .withColumn(\"fsclqrtr_label\", expr(\"concat('Q', quarter(date), ', ', year(date))\")) \\\n",
    "       .withColumn(\"fsclyr_id\", year(col(\"date\"))) \\\n",
    "       .withColumn(\"fsclyr_label\", year(col(\"date\")).cast(\"string\")) \\\n",
    "       .withColumn(\"fscldow\", dayofweek(col(\"date\"))) \\\n",
    "       .withColumn(\"fscldom\", dayofmonth(col(\"date\"))) \\\n",
    "       .withColumn(\"fscldoq\", expr(\"dayofmonth(date) + (quarter(date) - 1) * 30\")) \\\n",
    "       .withColumn(\"fscldoy\", expr(\"dayofyear(date)\")) \\\n",
    "       .withColumn(\"fsclwoy\", weekofyear(col(\"date\"))) \\\n",
    "       .withColumn(\"fsclmoy\", month(col(\"date\"))) \\\n",
    "       .withColumn(\"fsclqoy\", quarter(col(\"date\")))\n",
    "\n",
    "# Add Last Year and Two Years Ago Fiscal Dates\n",
    "df = df.withColumn(\"ly_fscldt_id\", expr(\"fscldt_id - 10000\")) \\\n",
    "       .withColumn(\"lly_fscldt_id\", expr(\"fscldt_id - 20000\"))\n",
    "\n",
    "# Add Season Labels\n",
    "df = df.withColumn(\"ssn_id\", expr(\"\"\"\n",
    "    CASE \n",
    "        WHEN fsclmoy IN (12,1,2) THEN concat('WINT', fsclyr_id) \n",
    "        WHEN fsclmoy IN (3,4,5) THEN concat('SPRG', fsclyr_id) \n",
    "        WHEN fsclmoy IN (6,7,8) THEN concat('SUMR', fsclyr_id) \n",
    "        ELSE concat('FALL', fsclyr_id) \n",
    "    END\n",
    "\"\"\")) \\\n",
    "       .withColumn(\"ssn_label\", expr(\"\"\"\n",
    "    CASE \n",
    "        WHEN fsclmoy IN (12,1,2) THEN 'Winter' \n",
    "        WHEN fsclmoy IN (3,4,5) THEN 'Spring' \n",
    "        WHEN fsclmoy IN (6,7,8) THEN 'Summer' \n",
    "        ELSE 'Fall' \n",
    "    END\n",
    "\"\"\"))\n",
    "\n",
    "# Show Sample Data\n",
    "df.show(10)\n",
    "\n",
    "# Save Table in Databricks\n",
    "df.write.mode(\"overwrite\").saveAsTable(\"processed_data.dim_date\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3789665760385030,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "processed_data date table creation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
